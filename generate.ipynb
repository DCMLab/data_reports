{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from dimcat import (\n",
    "    Corpus,  \n",
    "    Pipeline,\n",
    "    IsAnnotatedFilter,\n",
    "    CorpusGrouper, \n",
    "    PieceGrouper, \n",
    "    ModeGrouper, \n",
    "    ChordSymbolBigrams, \n",
    "    ChordSymbolUnigrams,\n",
    "    LocalKeySlicer,\n",
    ")\n",
    "from dimcat import __version__ as dimcat_version\n",
    "from ms3 import __version__ as ms3_version\n",
    "from git import Repo\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"~/romantic_piano_corpus\"\n",
    "\n",
    "repo = Repo(corpus_path)\n",
    "print(f\"{os.path.basename(corpus_path)} @ {repo.commit().hexsha[:7]}\")\n",
    "print(f\"dimcat version {dimcat_version}\")\n",
    "print(f\"ms3 version {ms3_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(directory=corpus_path)\n",
    "corpus.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metadata = corpus.data.metadata(from_tsv=True)\n",
    "print(f\"Concatenated 'metadata.tsv' files cover {len(all_metadata)} of the {len(corpus.data._score_ids())} scores.\")\n",
    "all_metadata.groupby(level=0).nth(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"VALUE COUNTS OF THE COLUMN 'annotators'\")\n",
    "all_metadata.annotators.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Composition dates range from {all_metadata.composed_start.min()} ({all_metadata.loc[all_metadata.composed_start.idxmin(), 'fnames']}) \"\n",
    "      f\"to {all_metadata.composed_end.max()} ({all_metadata.loc[all_metadata.composed_end.idxmax(), 'fnames']}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated = IsAnnotatedFilter().process_data(corpus)\n",
    "print(f\"Before: {len(corpus.indices[()])} IDs, after filtering: {len(annotated.indices[()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose here if you want to see stats for all or only for annotated scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected = corpus\n",
    "selected = annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_measures = selected.get_facet('measures')\n",
    "print(f\"{len(all_measures.index)} measures over {len(all_measures.groupby(level=[0,1]))} files.\")\n",
    "all_measures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribution of time signatures per XML measure (MC):\")\n",
    "all_measures.timesig.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes = selected.get_facet('notes')\n",
    "print(f\"{len(all_notes.index)} notes over {len(all_notes.groupby(level=[0,1]))} files.\")\n",
    "all_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_LAYOUT = {\n",
    " 'paper_bgcolor': '#FFFFFF',\n",
    " 'plot_bgcolor': '#FFFFFF',\n",
    " 'margin': {'l': 40, 'r': 0, 'b': 0, 't': 40, 'pad': 0},\n",
    " 'font': {'size': 15}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_notes(nl, group_col='midi', precise=True):\n",
    "    summed_durations = nl.groupby(group_col).duration_qb.sum()\n",
    "    summed_durations /= summed_durations.min()\n",
    "    if not precise:\n",
    "        # This simple trick reduces compute time but also precision:\n",
    "        # The rationale is to have the smallest value be slightly larger than 0.5 because\n",
    "        # if it was exactly 0.5 it would be rounded down by repeat_notes_according_to_weights()\n",
    "        summed_durations /= 1.9999999\n",
    "    return repeat_notes_according_to_weights(summed_durations)\n",
    "    \n",
    "def repeat_notes_according_to_weights(weights):\n",
    "    counts = weights.round().astype(int)\n",
    "    counts_reflecting_weights = []\n",
    "    for pitch, count in counts.iteritems():\n",
    "        counts_reflecting_weights.extend([pitch]*count)\n",
    "    return pd.Series(counts_reflecting_weights)\n",
    "\n",
    "grouped_notes = all_notes.groupby(level=0)\n",
    "weighted_midi = pd.concat([weight_notes(nl, 'midi', precise=False) for _, nl in grouped_notes], axis=1, keys=grouped_notes.groups.keys())\n",
    "weighted_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaxis=dict(tickmode= 'array',\n",
    "           tickvals= [12, 24, 36, 48, 60, 72, 84, 96],\n",
    "           ticktext = [\"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\"],\n",
    "           gridcolor='lightgrey',\n",
    "           )\n",
    "fig = px.violin(weighted_midi, labels=dict(variable='', value='pitch'), box=True, height=500) #, title=\"Distribution of pitches per corpus\"\n",
    "fig.update_layout(yaxis=yaxis, **STD_LAYOUT)\n",
    "fig.write_image(\"/home/hentsche/Documents/phd/romantic_piano_corpus_report/figures/ambitus_per_corpus.png\", scale=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_tpc = pd.concat([weight_notes(nl, 'tpc') for _, nl in grouped_notes], axis=1, keys=grouped_notes.groups.keys())\n",
    "weighted_tpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaxis=dict(\n",
    "    tickmode= 'array',\n",
    "    tickvals= [-12, -9, -6, -3, 0, 3, 6, 9, 12, 15, 18],\n",
    "    ticktext = [\"Dbb\", \"Bbb\", \"Gb\", \"Eb\", \"C\", \"A\", \"F#\", \"D#\", \"B#\", \"G##\", \"E##\"],\n",
    "    gridcolor='lightgrey',\n",
    "    zerolinecolor='lightgrey',\n",
    "    zeroline=True\n",
    "           )\n",
    "fig = px.violin(weighted_tpc, labels=dict(variable='', value='pitch class'), box=True, height=500)\n",
    "fig.update_layout(yaxis=yaxis, **STD_LAYOUT)\n",
    "fig.write_image(\"/home/hentsche/Documents/phd/romantic_piano_corpus_report/figures/tpc_per_corpus.png\", scale=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://plotly.com/python/violin/#ridgeline-plot\n",
    "fig = go.Figure()\n",
    "for corpus, data_line in weighted_tpc.iteritems():\n",
    "    fig.add_trace(go.Violin(x=data_line, name=corpus))\n",
    "\n",
    "fig.update_traces(side='positive', orientation='h', width=2, points=False)\n",
    "fig.update_layout(xaxis_showgrid=False, xaxis_zeroline=True, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=len(grouped_notes), cols=1, subplot_titles=list(grouped_notes.groups.keys()), shared_xaxes=True)\n",
    "for i, (corpus, notes) in enumerate(grouped_notes, 1):\n",
    "    tpc_durations = notes.groupby('tpc').duration_qb.sum()\n",
    "    tpc_durations /= tpc_durations.sum()\n",
    "    fig.add_trace(go.Scatter(x=tpc_durations.index, y=tpc_durations, name=corpus, mode='lines+markers'), row=i, col=1)\n",
    "\n",
    "#fig.update_traces(side='positive', orientation='h', width=2, points=False)\n",
    "fig.update_layout(**STD_LAYOUT, showlegend=False, height=800, width=300)\n",
    "fig.update_xaxes(gridcolor='lightgrey', zerolinecolor='lightgrey', tickmode='array', tickvals= [-12, -6, 0, 6, 12, 18],\n",
    "    ticktext = [\"Dbb\", \"Gb\", \"C\", \"F#\", \"B#\", \"E##\"],)\n",
    "fig.update_yaxes(showgrid=False, zeroline=False)\n",
    "fig.write_image(\"/home/hentsche/Documents/phd/romantic_piano_corpus_report/figures/tpc_line_per_corpus.png\", scale=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes and staves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribution of notes over staves:\")\n",
    "all_notes.staff.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribution of notes over staves for all pieces with more than two staves\\n\")\n",
    "for group, df in all_notes.groupby(level=[0,1]):\n",
    "    if (df.staff > 2).any():\n",
    "        print(group)\n",
    "        print(df.staff.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes[all_notes.staff > 2].groupby(level=[0,1]).staff.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Harmony labels\n",
    "\n",
    "All symbols, independent of the local key (the mode of which changes their semantics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_annotations = annotated.get_facet('expanded')\n",
    "all_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_chord = all_annotations.root.isna()\n",
    "print(f\"Concatenated annotation tables contains {all_annotations.shape[0]} rows. {no_chord.sum()} of them are not chords. Their values are:\")\n",
    "all_annotations.label[no_chord].value_counts(dropna=False).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_chords = all_annotations[~no_chord]\n",
    "print(f\"Corpus contains {all_chords.shape[0]} tokens and {len(all_chords.chord.unique())} types over {len(all_chords.groupby(level=[0,1]))} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#from ms3 import write_tsv\n",
    "#write_tsv(all_annotations[all_annotations.pedalend.notna()], './issues/pedalpoints.tsv', pre_process=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = all_metadata.set_index('fnames', append=True).reset_index(level=[1,2], drop=True)\n",
    "if selected == annotated:\n",
    "    summary = summary[summary.label_count > 0].copy()\n",
    "summary.index = summary.index.rename(['corpus', 'fname'])\n",
    "summary.length_qb = all_measures.groupby(level=[0,1]).act_dur.sum() * 4.0\n",
    "summary = pd.concat([summary,\n",
    "                     all_notes.groupby(level=[0,1]).size().rename('notes'),\n",
    "                    ], axis=1)\n",
    "summary.groupby(level=0).describe().dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_metadata = summary.groupby(level=0)\n",
    "n_pieces = corpus_metadata.size().rename('pieces')\n",
    "absolute_numbers = dict(\n",
    "    measures = corpus_metadata.last_mn.sum(),\n",
    "    length = corpus_metadata.length_qb.sum(),\n",
    "    notes = corpus_metadata.notes.sum(),\n",
    "    labels = corpus_metadata.label_count.sum(),\n",
    ")\n",
    "absolute = pd.DataFrame.from_dict(absolute_numbers)\n",
    "relative = absolute.div(n_pieces, axis=0).astype(float).round(1)\n",
    "complete_summary = pd.concat([pd.concat([n_pieces, absolute], axis=1), relative], axis=1, keys=['absolute', 'per piece'])\n",
    "complete_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Harmony labels\n",
    "## Unigrams\n",
    "For computing unigram statistics, the tokens need to be grouped by their occurrence within a major or a minor key because this changes their meaning. To that aim, the annotated corpus needs to be sliced into contiguous localkey segments which are then grouped into a major (`is_minor=False`) and a minor group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localkey_slices = LocalKeySlicer().process_data(annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_slices = ModeGrouper().process_data(localkey_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = ChordSymbolUnigrams(once_per_group=True).process_data(mode_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = {True: 'MINOR', False: 'MAJOR'}\n",
    "for (is_minor,), ugs in unigrams.iter():\n",
    "    print(f\"{modes[is_minor]} UNIGRAMS\\n{ugs.shape[0]} types, {ugs.sum()} tokens\")\n",
    "    print(ugs.head(20).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wise_unigrams = Pipeline([CorpusGrouper(), ChordSymbolUnigrams(once_per_group=True)]).process_data(mode_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wise_unigrams.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (is_minor, corpus_name), ugs in corpus_wise_unigrams.iter():\n",
    "    print(f\"{corpus_name} {modes[is_minor]} unigrams ({ugs.shape[0]} types, {ugs.sum()} tokens)\")\n",
    "    print(ugs.head(5).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_shared_between_corpora = {}\n",
    "for (is_minor, corpus_name), ugs in corpus_wise_unigrams.iter():\n",
    "    if is_minor in types_shared_between_corpora:\n",
    "        types_shared_between_corpora[is_minor] = types_shared_between_corpora[is_minor].intersection(ugs.index) \n",
    "    else:\n",
    "        types_shared_between_corpora[is_minor] = set(ugs.index)\n",
    "types_shared_between_corpora = {k: sorted(v, key=lambda x: unigrams.get()[(k, x)], reverse=True) for k, v in types_shared_between_corpora.items()}\n",
    "n_types = {k: len(v) for k, v in types_shared_between_corpora.items()}\n",
    "print(f\"Chords which occur in all corpora, sorted by descending global frequency:\\n{types_shared_between_corpora}\\nCounts: {n_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_wise_unigrams = Pipeline([PieceGrouper(), ChordSymbolUnigrams(once_per_group=True)]).process_data(mode_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_wise_unigrams.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_shared_between_pieces = {}\n",
    "for (is_minor, corpus_name), ugs in piece_wise_unigrams.iter():\n",
    "    if is_minor in types_shared_between_pieces:\n",
    "        types_shared_between_pieces[is_minor] = types_shared_between_pieces[is_minor].intersection(ugs.index) \n",
    "    else:\n",
    "        types_shared_between_pieces[is_minor] = set(ugs.index)\n",
    "print(types_shared_between_pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = ChordSymbolBigrams(once_per_group=True).process_data(mode_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = {True: 'MINOR', False: 'MAJOR'}\n",
    "for (is_minor,), ugs in bigrams.iter():\n",
    "    print(f\"{modes[is_minor]} BIGRAMS\\n{ugs.shape[0]} transition types, {ugs.sum()} tokens\")\n",
    "    print(ugs.head(20).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wise_bigrams = Pipeline([CorpusGrouper(), ChordSymbolBigrams(once_per_group=True)]).process_data(mode_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wise_bigrams.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (is_minor, corpus_name), ugs in corpus_wise_bigrams.iter():\n",
    "    print(f\"{corpus_name} {modes[is_minor]} bigrams ({ugs.shape[0]} transition types, {ugs.sum()} tokens)\")\n",
    "    print(ugs.head(5).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_corpus_unigrams = {group: (100 * ugs / ugs.sum()).round(1).rename(\"frequency\") for group, ugs in corpus_wise_unigrams.iter()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_from_shared_types = {\n",
    "    False: {},\n",
    "    True: {}\n",
    "}\n",
    "for (is_minor, corpus_name), bgs in corpus_wise_bigrams.iter():\n",
    "    transitions_normalized_per_from = bgs.groupby(level=\"from\").apply(lambda S: (100 * S / S.sum()).round(1))\n",
    "    most_frequent_transition_per_from = transitions_normalized_per_from.rename('fraction').reset_index(level=1).groupby(level=0).nth(0)\n",
    "    most_frequent_transition_per_shared = most_frequent_transition_per_from.loc[types_shared_between_corpora[is_minor]]\n",
    "    unigram_frequency_of_shared = normalized_corpus_unigrams[(is_minor, corpus_name)].loc[types_shared_between_corpora[is_minor]]\n",
    "    combined = pd.concat([unigram_frequency_of_shared, most_frequent_transition_per_shared], axis=1)\n",
    "    transitions_from_shared_types[is_minor][corpus_name] = combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(transitions_from_shared_types[False].values(), keys=transitions_from_shared_types[False].keys(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(transitions_from_shared_types[True].values(), keys=transitions_from_shared_types[False].keys(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_wise_bigrams = Pipeline([PieceGrouper(), ChordSymbolBigrams(once_per_group=True)]).process_data(mode_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_wise_bigrams.get()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dimcat",
   "language": "python",
   "name": "dimcat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
